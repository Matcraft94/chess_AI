{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHess RL Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se está utilizando el dispositivo cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Se está utilizando el dispositivo',device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el ambiente del juego y las respectivas reglas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_tensor(board):\n",
    "    piece_map = board.piece_map()\n",
    "    board_tensor = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "\n",
    "    for pos, piece in piece_map.items():\n",
    "        rank, file = chess.square_rank(pos), chess.square_file(pos)\n",
    "        piece_idx = piece.piece_type - 1 + (6 if piece.color == chess.BLACK else 0)\n",
    "        board_tensor[piece_idx, rank, file] = 1\n",
    "\n",
    "    return board_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnvironment:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board.reset()\n",
    "        return self.board_to_array(self.board)\n",
    "    \n",
    "    def board_to_array(self, board):\n",
    "        # Convierte el tablero en una matriz 8x8x12 que representa las piezas y sus posiciones.\n",
    "        piece_symbols = \"PRNBQKprnbqk\"\n",
    "        piece_indices = {symbol: i for i, symbol in enumerate(piece_symbols)}\n",
    "        board_matrix = np.zeros((12, 8, 8))\n",
    "\n",
    "        for i in range(64):\n",
    "            piece = board.piece_at(i)\n",
    "            if piece:\n",
    "                piece_index = piece_indices[piece.symbol()]\n",
    "                row, col = divmod(i, 8)\n",
    "                board_matrix[piece_index, row, col] = 1\n",
    "\n",
    "        return board_matrix\n",
    "\n",
    "    def step(self, move):\n",
    "        # Aplicar el movimiento al tablero y devolver el nuevo estado, recompensa y si el juego ha terminado.\n",
    "        game_over = False\n",
    "        reward = 0\n",
    "\n",
    "        if move in self.legal_moves():\n",
    "            self.board.push(move)\n",
    "            game_over = self.board.is_game_over()\n",
    "            if game_over:\n",
    "                reward = self.get_reward()\n",
    "        else:\n",
    "            raise ValueError(\"Illegal move\")\n",
    "\n",
    "        next_state = self.board_to_array(self.board)\n",
    "        return next_state, reward, game_over\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.board.legal_moves)\n",
    "\n",
    "    # def is_game_over(self):\n",
    "    #     return self.board.is_game_over()\n",
    "    def is_game_over(self):  # Modifica la función para que no requiera argumentos adicionales\n",
    "        return self.board.is_game_over()\n",
    "\n",
    "    def get_reward(self):\n",
    "        result = self.board.result()\n",
    "        if result == \"1-0\":  # White wins\n",
    "            return 1\n",
    "        elif result == \"0-1\":  # Black wins\n",
    "            return -1\n",
    "        else:  # Draw\n",
    "            return 0\n",
    "    \n",
    "    def get_state(self):\n",
    "        return board_to_tensor(self.board)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class ChessNetwork(nn.Module):\n",
    "    def __init__(self, num_residual_blocks=3, num_channels=256):\n",
    "        super(ChessNetwork, self).__init__()\n",
    "\n",
    "        self.conv_input = nn.Conv2d(12, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_residual_blocks)])\n",
    "\n",
    "        self.conv_policy = nn.Conv2d(num_channels, 2, kernel_size=1)\n",
    "        self.bn_policy = nn.BatchNorm2d(2)\n",
    "        self.fc_policy = nn.Linear(2 * 8 * 8, 4096)  # 4096 es el número máximo de movimientos legales en ajedrez\n",
    "\n",
    "        self.conv_value = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "        self.bn_value = nn.BatchNorm2d(1)\n",
    "        self.fc_value1 = nn.Linear(8 * 8, 256)\n",
    "        self.fc_value2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
    "        x = self.residual_blocks(x)\n",
    "\n",
    "        policy = F.relu(self.bn_policy(self.conv_policy(x)))\n",
    "        policy = policy.view(-1, 2 * 8 * 8)\n",
    "        policy = F.softmax(self.fc_policy(policy), dim=-1)\n",
    "\n",
    "        value = F.relu(self.bn_value(self.conv_value(x)))\n",
    "        value = value.view(-1, 8 * 8)\n",
    "        value = F.relu(self.fc_value1(value))\n",
    "        value = torch.tanh(self.fc_value2(value))\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChessNetwork(nn.Module):\n",
    "#     def __init__(self, num_res_blocks=6, num_channels=256):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Input layer\n",
    "#         self.conv_input = nn.Conv2d(12, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn_input = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "#         # Residual blocks\n",
    "#         self.residual_blocks = nn.Sequential(\n",
    "#             *[ResidualBlock(num_channels) for _ in range(num_res_blocks)]\n",
    "#         )\n",
    "\n",
    "#         # Policy head\n",
    "#         self.conv_policy = nn.Conv2d(num_channels, 4672, kernel_size=1)\n",
    "#         self.bn_policy = nn.BatchNorm2d(4672)  # Change to BatchNorm2d\n",
    "#         self.fc_policy = nn.Linear(4672 * 8 * 8, 4672)\n",
    "\n",
    "#         # Value head\n",
    "#         self.conv_value = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "#         self.bn_value = nn.BatchNorm2d(1)  # Change to BatchNorm2d\n",
    "#         self.fc_value_1 = nn.Linear(8 * 8, 256)\n",
    "#         self.fc_value_2 = nn.Linear(256, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn_input(self.conv_input(x)))\n",
    "#         x = self.residual_blocks(x)\n",
    "\n",
    "#         policy = F.relu(self.bn_policy(self.conv_policy(x)))\n",
    "#         policy = policy.view(-1, 4672 * 8 * 8)\n",
    "#         policy = F.softmax(self.fc_policy(policy), dim=1)\n",
    "\n",
    "#         value = F.relu(self.bn_value(self.conv_value(x)))\n",
    "#         value = value.view(-1, 8 * 8)\n",
    "#         value = F.relu(self.fc_value_1(value))\n",
    "#         value = torch.tanh(self.fc_value_2(value))\n",
    "\n",
    "#         return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChessNetworkSmall(nn.Module):\n",
    "#     def __init__(self, num_res_blocks=3, num_channels=128):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Input layer\n",
    "#         self.conv_input = nn.Conv2d(12, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn_input = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "#         # Residual blocks\n",
    "#         self.residual_blocks = nn.Sequential(\n",
    "#             *[ResidualBlock(num_channels) for _ in range(num_res_blocks)]\n",
    "#         )\n",
    "\n",
    "#         # Policy head\n",
    "#         self.conv_policy = nn.Conv2d(num_channels, 128, kernel_size=1)\n",
    "#         self.bn_policy = nn.BatchNorm2d(128)\n",
    "#         self.fc_policy = nn.Linear(128 * 8 * 8, 4672)\n",
    "\n",
    "#         # Value head\n",
    "#         self.conv_value = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "#         self.bn_value = nn.BatchNorm2d(1)\n",
    "#         self.fc_value_1 = nn.Linear(8 * 8, 128)\n",
    "#         self.fc_value_2 = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.bn_input(self.conv_input(x)))\n",
    "#         x = self.residual_blocks(x)\n",
    "\n",
    "#         policy = F.relu(self.bn_policy(self.conv_policy(x)))\n",
    "#         policy = policy.view(-1, 128 * 8 * 8)\n",
    "#         policy = F.softmax(self.fc_policy(policy), dim=1)\n",
    "\n",
    "#         value = F.relu(self.bn_value(self.conv_value(x)))\n",
    "#         value = value.view(-1, 8 * 8)\n",
    "#         value = F.relu(self.fc_value_1(value))\n",
    "#         value = torch.tanh(self.fc_value_2(value))\n",
    "\n",
    "#         return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessNetworkreduced(nn.Module):\n",
    "    def __init__(self, num_residual_blocks=1, num_channels=64):\n",
    "        super(ChessNetworkreduced, self).__init__()\n",
    "\n",
    "        self.conv_input = nn.Conv2d(12, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_residual_blocks)])\n",
    "\n",
    "        self.conv_policy = nn.Conv2d(num_channels, 2, kernel_size=1)\n",
    "        self.bn_policy = nn.BatchNorm2d(2)\n",
    "        self.fc_policy = nn.Linear(2 * 8 * 8, 4096)  # 4096 es el número máximo de movimientos legales en ajedrez\n",
    "\n",
    "        self.conv_value = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "        self.bn_value = nn.BatchNorm2d(1)\n",
    "        self.fc_value1 = nn.Linear(8 * 8, 256)\n",
    "        self.fc_value2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
    "        x = self.residual_blocks(x)\n",
    "\n",
    "        policy = F.relu(self.bn_policy(self.conv_policy(x)))\n",
    "        policy = policy.view(-1, 2 * 8 * 8)\n",
    "        policy = F.softmax(self.fc_policy(policy), dim=-1)\n",
    "\n",
    "        value = F.relu(self.bn_value(self.conv_value(x)))\n",
    "        value = value.view(-1, 8 * 8)\n",
    "        value = F.relu(self.fc_value1(value))\n",
    "        value = torch.tanh(self.fc_value2(value))\n",
    "\n",
    "        return policy, value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, parent, prior, action):\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.prior = prior\n",
    "\n",
    "    def expand(self, env, action_probs):\n",
    "        for move, prob in action_probs.items():\n",
    "            if move not in self.children:\n",
    "                self.children[move] = MCTSNode(self, prob, move)\n",
    "\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select_child(self):\n",
    "        C = 1.0  # Parámetro de exploración\n",
    "        best_score = None\n",
    "        best_action = None\n",
    "        best_child = None\n",
    "\n",
    "        for action, child in self.children.items():\n",
    "            score = child.get_ucb_score(C)\n",
    "            if best_score is None or score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "                best_child = child\n",
    "\n",
    "        return best_action, best_child\n",
    "\n",
    "    def get_ucb_score(self, C):\n",
    "        Q = self.value()  # Valor medio\n",
    "        U = C * self.prior * math.sqrt(self.parent.visit_count) / (1 + self.visit_count)  # Potencial de mejora\n",
    "        return Q + U\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "        self.visit_count += 1\n",
    "        self.value_sum += value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_index(move):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    return from_square * 64 + to_square\n",
    "\n",
    "def index_to_move(index):\n",
    "    from_square = index // 64\n",
    "    to_square = index % 64\n",
    "    return chess.Move(from_square, to_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts(model, env, num_simulations, temperature):\n",
    "    root = MCTSNode(None, 1.0, None)\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        node = root\n",
    "        board_copy = env.board.copy()\n",
    "\n",
    "        # Selección y expansión\n",
    "        while node.is_expanded():\n",
    "            action, node = node.select_child()\n",
    "            board_copy.push(action)\n",
    "\n",
    "        # Simulación\n",
    "        if not env.is_game_over():\n",
    "            legal_moves = list(board_copy.legal_moves)\n",
    "            state = env.board_to_array(board_copy)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            policy, value = model(state_tensor)\n",
    "            policy = policy.cpu().detach().numpy().flatten()\n",
    "\n",
    "            if len(legal_moves) > 0:  # Verifica si hay al menos un movimiento legal\n",
    "                action_probs = {move: policy[move_to_index(move)] for move in legal_moves}\n",
    "                node.expand(env, action_probs)\n",
    "            else:\n",
    "                action_probs = {}  # Inicializa un diccionario vacío si no hay movimientos legales\n",
    "\n",
    "            value = value.item()\n",
    "        else:\n",
    "            value = env.get_reward(board_copy)\n",
    "\n",
    "        # Retroceso\n",
    "        node.backpropagate(value)\n",
    "\n",
    "    # Calcula la política final a partir del número de visitas de las acciones.\n",
    "    legal_moves = list(env.board.legal_moves)  # Añade esta línea para obtener una lista de movimientos legales\n",
    "    visit_counts = np.array([root.children.get(action, 0).visit_count for action in legal_moves])  # Itera sobre los movimientos legales en lugar de usar num_legal_moves\n",
    "\n",
    "    if temperature == 0:\n",
    "        action_idx = np.argmax(visit_counts)\n",
    "        policy = np.zeros_like(visit_counts)\n",
    "        policy[action_idx] = 1\n",
    "    else:\n",
    "        visit_counts = visit_counts ** (1 / temperature)\n",
    "        policy = visit_counts / visit_counts.sum()\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_game(model, env, num_mcts_simulations, temperature, return_result=False):\n",
    "#     states = []\n",
    "#     policy_targets = []\n",
    "#     value_targets = []\n",
    "\n",
    "#     while not env.board.is_game_over():\n",
    "#         # Calcular la política objetivo utilizando MCTS.\n",
    "#         policy = run_mcts(model, env, num_mcts_simulations, temperature)\n",
    "\n",
    "#         legal_moves = list(env.legal_moves())  # Obtiene los movimientos legales antes de verificar su tamaño\n",
    "#         if len(legal_moves) > 0:  # Verifica si hay al menos un movimiento legal\n",
    "#             # print(f\"Policy: {policy}\")  # Agrega esta línea para verificar el contenido de la política\n",
    "#             action = np.random.choice(len(policy), p=policy)\n",
    "#             move = legal_moves[action]\n",
    "#             env.board.push(move)\n",
    "\n",
    "#             states.append(env.get_state().copy())\n",
    "#             policy_targets.append(policy.copy())\n",
    "\n",
    "#     result = env.board.result()\n",
    "\n",
    "#     # Calcular las recompensas basadas en el resultado.\n",
    "#     if result == \"1-0\":\n",
    "#         value_targets = [1] * len(states)\n",
    "#     elif result == \"0-1\":\n",
    "#         value_targets = [-1] * len(states)\n",
    "#     else:\n",
    "#         value_targets = [0] * len(states)\n",
    "\n",
    "#     if return_result:\n",
    "#         return result\n",
    "#     else:\n",
    "#         return states, policy_targets, value_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, env, num_mcts_simulations, temperature, return_result=False):\n",
    "    states = []\n",
    "    policy_targets = []\n",
    "    value_targets = []\n",
    "\n",
    "    while not env.board.is_game_over():\n",
    "        # Calcular la política objetivo utilizando MCTS.\n",
    "        policy = run_mcts(model, env, num_mcts_simulations, temperature)\n",
    "\n",
    "        legal_moves = list(env.legal_moves())  # Obtiene los movimientos legales antes de verificar su tamaño\n",
    "        if len(legal_moves) > 0:  # Verifica si hay al menos un movimiento legal\n",
    "            # print(f\"Policy: {policy}\")  # Agrega esta línea para verificar el contenido de la política\n",
    "            action = np.random.choice(len(policy), p=policy)\n",
    "            move = legal_moves[action]\n",
    "            env.board.push(move)\n",
    "\n",
    "            states.append(env.get_state().copy())\n",
    "\n",
    "            # Asegúrate de que todas las políticas objetivo tengan la misma longitud que el número máximo de movimientos legales\n",
    "            padded_policy = np.zeros(4096)\n",
    "            padded_policy[:len(policy)] = policy\n",
    "            policy_targets.append(padded_policy)\n",
    "\n",
    "    result = env.board.result()\n",
    "\n",
    "    # Calcular las recompensas basadas en el resultado.\n",
    "    if result == \"1-0\":\n",
    "        value_targets = [1] * len(states)\n",
    "    elif result == \"0-1\":\n",
    "        value_targets = [-1] * len(states)\n",
    "    else:\n",
    "        value_targets = [0] * len(states)\n",
    "\n",
    "    if return_result:\n",
    "        return result\n",
    "    else:\n",
    "        return states, policy_targets, value_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_games(model, num_games=100, num_mcts_simulations=800, temperature=1.0):\n",
    "def play_games(model, num_games=10, num_mcts_simulations=8, temperature=1.0):\n",
    "    states, policy_targets, value_targets = [], [], []\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        env = ChessEnvironment()\n",
    "        game_states, game_policies, game_values = play_game(model, env, num_mcts_simulations, temperature)\n",
    "        states.extend(game_states)\n",
    "        policy_targets.extend(game_policies)\n",
    "        value_targets.extend(game_values)\n",
    "\n",
    "    return states, policy_targets, value_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_loss(model, states, policy_targets, value_targets, device):\n",
    "#     states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    \n",
    "#     # Ajustar la forma de las acciones objetivo para que coincidan con la salida de la política del modelo\n",
    "#     policy_targets = torch.tensor(policy_targets, dtype=torch.long).to(device)\n",
    "#     policy_targets_one_hot = torch.zeros(policy_targets.size(0), 4096, device=device)\n",
    "#     policy_targets_one_hot.scatter_(1, policy_targets.unsqueeze(1), 1.0)\n",
    "    \n",
    "#     value_targets = torch.tensor(np.array(value_targets), dtype=torch.float32).to(device)\n",
    "\n",
    "#     policies, values = model(states)\n",
    "\n",
    "#     policy_loss = F.binary_cross_entropy(policies, policy_targets_one_hot)\n",
    "#     value_loss = F.mse_loss(values.view(-1), value_targets)\n",
    "\n",
    "#     total_loss = policy_loss + value_loss\n",
    "#     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, states, policy_targets, value_targets, device):\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "    policy_targets = torch.tensor(np.array(policy_targets), dtype=torch.float32).to(device)\n",
    "    value_targets = torch.tensor(np.array(value_targets), dtype=torch.float32).to(device)\n",
    "\n",
    "    policies, values = model(states)\n",
    "\n",
    "    # Utilizar nll_loss en lugar de binary_cross_entropy\n",
    "    policy_loss = F.nll_loss(torch.log(policies), torch.argmax(policy_targets, dim=1))\n",
    "    value_loss = F.mse_loss(values.view(-1), value_targets)\n",
    "\n",
    "    total_loss = policy_loss + value_loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_game(model, env, num_mcts_simulations):\n",
    "    while not env.is_game_over():\n",
    "        state = env.board_to_array(env.board)\n",
    "        policy, _ = run_mcts(model, env, num_mcts_simulations, temperature=0)\n",
    "\n",
    "        action = np.argmax(policy)\n",
    "        legal_moves = list(env.legal_moves())\n",
    "        move = legal_moves[action]\n",
    "\n",
    "        env.step(move)\n",
    "\n",
    "    return env.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_and_save_model(model, save_dir=\"saved_models\", num_evaluation_games=100, num_mcts_simulations=800):\n",
    "def evaluate_and_save_model(model, save_dir=\"saved_models\", num_evaluation_games=10, num_mcts_simulations=8):\n",
    "    win_count = 0\n",
    "    draw_count = 0\n",
    "    loss_count = 0\n",
    "\n",
    "    for _ in range(num_evaluation_games):\n",
    "        env = ChessEnvironment()\n",
    "        game_result = evaluate_game(model, env, num_mcts_simulations)\n",
    "\n",
    "        if game_result == 1:\n",
    "            win_count += 1\n",
    "        elif game_result == 0:\n",
    "            draw_count += 1\n",
    "        else:\n",
    "            loss_count += 1\n",
    "\n",
    "    win_rate = win_count / num_evaluation_games\n",
    "    draw_rate = draw_count / num_evaluation_games\n",
    "    loss_rate = loss_count / num_evaluation_games\n",
    "\n",
    "    print(f\"Evaluation results: {num_evaluation_games} games played\")\n",
    "    print(f\"Win rate: {win_rate:.2f}, Draw rate: {draw_rate:.2f}, Loss rate: {loss_rate:.2f}\")\n",
    "\n",
    "    # Guardar el modelo si es mejor que el modelo anteriormente guardado.\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"model_win_rate_{win_rate:.2f}.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_games=10):\n",
    "    model.eval()  # Cambiar el modelo a modo de evaluación.\n",
    "\n",
    "    num_wins = 0\n",
    "    num_draws = 0\n",
    "    num_losses = 0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        env = ChessEnvironment()\n",
    "        # game_result = play_game(model, env, num_mcts_simulations=50, temperature=0)\n",
    "        game_result = play_game(model, env, num_mcts_simulations=50, temperature=0, return_result=True)\n",
    "\n",
    "        if game_result == \"1-0\":\n",
    "            num_wins += 1\n",
    "        elif game_result == \"0-1\":\n",
    "            num_losses += 1\n",
    "        elif game_result == \"1/2-1/2\":\n",
    "            num_draws += 1\n",
    "\n",
    "    win_rate = num_wins / num_games\n",
    "    draw_rate = num_draws / num_games\n",
    "    loss_rate = num_losses / num_games\n",
    "\n",
    "    return win_rate, draw_rate, loss_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, optimizer, epochs, evaluation_interval):\n",
    "#     for epoch in range(epochs):\n",
    "#         # Generar partidas de autojugabilidad.\n",
    "#         states, actions, rewards = play_games(model)\n",
    "        \n",
    "#         # Calcular la pérdida y actualizar los pesos de la red neuronal.\n",
    "#         loss = compute_loss(model, states, actions, rewards, device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Evaluar y guardar el modelo, si es necesario.\n",
    "#         if epoch % evaluation_interval == 0:\n",
    "#             evaluate_and_save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, optimizer, epochs, evaluation_interval):\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         # Generar partidas de autojugabilidad.\n",
    "#         states, actions, rewards = play_games(model)\n",
    "        \n",
    "#         # Calcular la pérdida y actualizar los pesos de la red neuronal.\n",
    "#         loss = compute_loss(model, states, actions, rewards, device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Evaluar y guardar el modelo, si es necesario.\n",
    "#         if epoch % evaluation_interval == 0:\n",
    "#             evaluate_and_save_model(model)\n",
    "        \n",
    "#             # Actualizar la barra de progreso con la información de la época actual y la pérdida.\n",
    "#             tqdm.set_description(f\"Epoch {epoch}, Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, evaluation_interval):\n",
    "    losses = []\n",
    "    min_loss = float(\"inf\")\n",
    "    min_epoch = 0  # Época en la que se alcanzó el mínimo de pérdida.\n",
    "    \n",
    "    progress_bar = tqdm(range(epochs))  # Guarda el objeto tqdm en la variable progress_bar\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        # Generar partidas de autojugabilidad.\n",
    "        states, actions, rewards = play_games(model)\n",
    "        \n",
    "        # Calcular la pérdida y actualizar los pesos de la red neuronal.\n",
    "        loss = compute_loss(model, states, actions, rewards, device)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Añadir la pérdida a la lista de pérdidas.\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Evaluar y guardar el modelo en intervalos regulares.\n",
    "        if (epoch + 1) % evaluation_interval == 0:\n",
    "            win_rate, draw_rate, loss_rate = evaluate(model)\n",
    "            print(f\"Evaluation results: {evaluation_interval} games played\")\n",
    "            print(f\"Win rate: {win_rate:.2f}, Draw rate: {draw_rate:.2f}, Loss rate: {loss_rate:.2f}\")\n",
    "\n",
    "            if loss.item() < min_loss:\n",
    "                min_loss = loss.item()\n",
    "                torch.save(model.state_dict(), f\"saved_models\\model_win_rate_{win_rate:.2f}.pt\")\n",
    "                min_epoch = epoch\n",
    "\n",
    "        # Actualizar la barra de progreso con la información de la época actual y la pérdida.\n",
    "        progress_bar.set_description(f\"Epoch {epoch}, Loss {loss.item():.4f}\")\n",
    "    \n",
    "    # Graficar la pérdida a lo largo de las épocas.\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    \n",
    "    # Indicar dónde se alcanzó el mínimo de pérdida.\n",
    "    plt.axvline(x=min_epoch, color='r', linestyle='--')\n",
    "    plt.text(min_epoch, min_loss, f\"Min Loss: {min_loss:.4f}\", color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo\n",
    "model = ChessNetworkreduced(num_residual_blocks=4, num_channels=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 128, 8, 8]          13,952\n",
      "       BatchNorm2d-2            [-1, 128, 8, 8]             256\n",
      "            Conv2d-3            [-1, 128, 8, 8]         147,584\n",
      "       BatchNorm2d-4            [-1, 128, 8, 8]             256\n",
      "            Conv2d-5            [-1, 128, 8, 8]         147,584\n",
      "       BatchNorm2d-6            [-1, 128, 8, 8]             256\n",
      "     ResidualBlock-7            [-1, 128, 8, 8]               0\n",
      "            Conv2d-8            [-1, 128, 8, 8]         147,584\n",
      "       BatchNorm2d-9            [-1, 128, 8, 8]             256\n",
      "           Conv2d-10            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-11            [-1, 128, 8, 8]             256\n",
      "    ResidualBlock-12            [-1, 128, 8, 8]               0\n",
      "           Conv2d-13            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-14            [-1, 128, 8, 8]             256\n",
      "           Conv2d-15            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-16            [-1, 128, 8, 8]             256\n",
      "    ResidualBlock-17            [-1, 128, 8, 8]               0\n",
      "           Conv2d-18            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
      "           Conv2d-20            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-21            [-1, 128, 8, 8]             256\n",
      "    ResidualBlock-22            [-1, 128, 8, 8]               0\n",
      "           Conv2d-23              [-1, 2, 8, 8]             258\n",
      "      BatchNorm2d-24              [-1, 2, 8, 8]               4\n",
      "           Linear-25                 [-1, 4096]         528,384\n",
      "           Conv2d-26              [-1, 1, 8, 8]             129\n",
      "      BatchNorm2d-27              [-1, 1, 8, 8]               2\n",
      "           Linear-28                  [-1, 256]          16,640\n",
      "           Linear-29                    [-1, 1]             257\n",
      "================================================================\n",
      "Total params: 1,742,602\n",
      "Trainable params: 1,742,602\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.41\n",
      "Params size (MB): 6.65\n",
      "Estimated Total Size (MB): 8.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Asume un tablero de ajedrez de 8x8 y 12 canales, que representan las diferentes piezas\n",
    "input_shape = (12, 8, 8)\n",
    "summary(model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenar el modelo\n",
    "# epochs = 20\n",
    "# evaluation_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit train(model, optimizer, epochs, evaluation_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 20\n",
    "# evaluation_interval = 5\n",
    "# num_mcts_simulations = 50\n",
    "# learning_rate = 1e-3\n",
    "# batch_size = 32\n",
    "\n",
    "# min_loss = float('inf')\n",
    "\n",
    "# # Optmizador\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_history = []\n",
    "\n",
    "# # Envolver el rango de épocas con tqdm para mostrar una barra de progreso\n",
    "# for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "#     epoch_losses = []\n",
    "#     states, policy_targets, value_targets = play_games(model, num_mcts_simulations=num_mcts_simulations)\n",
    "#     dataset = TensorDataset(\n",
    "#         torch.tensor(np.array(states), dtype=torch.float32),\n",
    "#         torch.tensor(np.array(policy_targets), dtype=torch.float32),\n",
    "#         torch.tensor(np.array(value_targets), dtype=torch.float32)\n",
    "#     )\n",
    "#     data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     model.train()\n",
    "#     pbar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "#     for state_batch, policy_target_batch, value_target_batch in pbar:\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = compute_loss(model, state_batch, policy_target_batch, value_target_batch, device)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         pbar.set_postfix({\"Loss\": loss.item()})\n",
    "#         epoch_losses.append(loss.item())\n",
    "\n",
    "#     epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     loss_history.append(epoch_loss)\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n",
    "\n",
    "#     if (epoch + 1) % evaluation_interval == 0:\n",
    "#         win_rate, draw_rate, loss_rate = evaluate(model)\n",
    "#         print(f\"Epoch {epoch + 1}, Win rate: {win_rate:.2f}, Draw rate: {draw_rate:.2f}, Loss rate: {loss_rate:.2f}\")\n",
    "\n",
    "#         if win_rate + draw_rate > min_loss:\n",
    "#             min_loss = win_rate + draw_rate\n",
    "#             min_epoch = epoch + 1\n",
    "#             torch.save(model.state_dict(), f\"saved_models/model_epoch_{epoch + 1}_win_rate_{win_rate:.2f}_draw_rate_{draw_rate:.2f}.pt\")\n",
    "#             print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "# print(f\"Best model found at epoch {min_epoch}\")\n",
    "\n",
    "# # Graficar la pérdida en función de las épocas\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(range(1, num_epochs + 1), loss_history, label=\"Loss\")\n",
    "# plt.scatter(min_epoch, loss_history[min_epoch - 1], color='red', label=f\"Min Loss at Epoch {min_epoch}\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Training Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 73/73 [00:01<00:00, 56.99it/s, Loss=3.89]\n",
      "Training:   1%|          | 1/100 [03:12<5:17:51, 192.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.424942718793268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 136/136 [00:01<00:00, 71.08it/s, Loss=0.311]\n",
      "Training:   2%|▏         | 2/100 [09:16<7:59:17, 293.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.8735010242637467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 112/112 [00:01<00:00, 70.07it/s, Loss=2.52]\n",
      "Training:   3%|▎         | 3/100 [14:29<8:08:41, 302.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2.935762699161257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 107/107 [00:01<00:00, 77.09it/s, Loss=2.77]\n",
      "Training:   4%|▍         | 4/100 [18:03<7:07:39, 267.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 3.484287157236973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 50/50 [00:00<00:00, 63.70it/s, Loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2.8299418473243714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 5/100 [25:09<8:34:09, 324.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Win rate: 0.00, Draw rate: 1.00, Loss rate: 0.00\n",
      "Model saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 106/106 [00:01<00:00, 76.37it/s, Loss=2.95]\n",
      "Training:   6%|▌         | 6/100 [28:47<7:31:47, 288.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 3.346702566686666\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\")  # Inicializar min_loss con un valor alto\n",
    "min_epoch = 0  # Inicializar min_epoch\n",
    "\n",
    "num_epochs = 100\n",
    "evaluation_interval = 5\n",
    "num_mcts_simulations = 50\n",
    "learning_rate = 1e-3\n",
    "batch_size = 10#32\n",
    "\n",
    "# Optmizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "# Envolver el rango de épocas con tqdm para mostrar una barra de progreso\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    epoch_losses = []\n",
    "    states, policy_targets, value_targets = play_games(model, num_mcts_simulations=num_mcts_simulations)\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(np.array(states), dtype=torch.float32),\n",
    "        torch.tensor(np.array(policy_targets), dtype=torch.float32),\n",
    "        torch.tensor(np.array(value_targets), dtype=torch.float32)\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "    for state_batch, policy_target_batch, value_target_batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, state_batch, policy_target_batch, value_target_batch, device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    if (epoch + 1) % evaluation_interval == 0:\n",
    "        win_rate, draw_rate, loss_rate = evaluate(model)\n",
    "        print(f\"Epoch {epoch + 1}, Win rate: {win_rate:.2f}, Draw rate: {draw_rate:.2f}, Loss rate: {loss_rate:.2f}\")\n",
    "\n",
    "        current_loss = loss_rate  # Asumir que loss_rate representa la pérdida actual\n",
    "        if current_loss < min_loss:\n",
    "            min_loss = current_loss\n",
    "            min_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), f\"saved_models/model_epoch_{epoch + 1}_win_rate_{win_rate:.2f}_draw_rate_{draw_rate:.2f}.pt\")\n",
    "            print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "print(f\"Best model found at epoch {min_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la pérdida en función de las épocas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), loss_history, label=\"Loss\")\n",
    "plt.scatter(min_epoch, loss_history[min_epoch - 1], color='red', label=f\"Min Loss at Epoch {min_epoch}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
